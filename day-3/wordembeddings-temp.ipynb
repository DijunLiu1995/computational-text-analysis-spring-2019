{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to word embeddings\n",
    "\n",
    "So far we have focused on bag-of-word approaches i.e representations of text as a vector of word frequencies. An alternative formalization of text consists in representing the words (or bi-grams, phrases, etc) themselves as vectors. A _word vector_ has no meaning per se, but it is informative of the _context_ in which the word is used. This vector representation can become very close to the semantic meaning of the word. Combined with simple vector operations, these representations can used to find synonyms, to test analogies, etc. Word vectors can also be used in any subsequent task (dictionary methods, classification, etc) as features instead of the simple word frequencies in the classical bag-of-words approach.\n",
    "\n",
    "In this notebook we are going to construct word embeddings using neural networks. The spirit of the method is to use 'prediction as an excuse': either predict a target word conditional on its surrounding words (_continuous-bag-of-words_) or predict surrounding words conditional on the target (_skip-gram_). What we care about is not the final output but the _hidden layer_ projection from a two-layers neural network designed to solve that prediction problem (see skip-gram diagram below from Mikolov et al. 2013). \n",
    "\n",
    "<img src='img/wordembeddings_diagram.png' />\n",
    "\n",
    "If we choose the hidden layer to have $K$ hidden neurons then each target word is represented by a $K$-dimensional vector of hidden outputm which we call _embedding_. In practice $K$ should be between 100 and 300, but that really depends on the vocabulary size. For the purpose of learning we are going to apply the famous skip-gram Google's Word2Vec approach (Mikolov et al. 2013) to a corpus that is typically _too small_ so that we reduce our word representations to vectors of 30 dimensions. Keep in mind that the typical required vocabulary size is at least half a million of unique tokens.\n",
    "\n",
    "Last note on Word2Vec: it is very powerful! When trained on very large corpora (like all of English Wikipedia) it can perform very strong analogies such as finding that the vector corresponding the most to the output of the operation 'king' - 'man' + 'woman' is 'queen'. There exist alternative packages such as GloVe (Stanford NLP) or FastText."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We are going to embed the vocabulary from the corpus of Jane Austen's books we encountered on day 1. First, let's read the files into a single string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "import os\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "import glob\n",
    "fnames = os.path.join(DATA_DIR, 'austen', '*.txt')\n",
    "fnames = glob.glob(fnames)\n",
    "raw = ''\n",
    "for fname in fnames:\n",
    "    with codecs.open(fname, \"r\", encoding='utf-8-sig', errors='ignore') as f:\n",
    "        t = f.read()\n",
    "        raw += t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg EBook of Emma, by Jane Austen\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and with\\r\\nalmost no restrictions whatsoever.  You may copy it, give it away or\\r\\nre-use it under the terms of the Project Gutenberg License included\\r\\nwith this eBook or online at www.gutenberg.org\\r\\n\\r\\n\\r\\nTitle: Emma\\r\\n\\r\\nAuthor: Jane Austen\\r\\n\\r\\nRelease Date: August, 1994  [Etext #158]\\r\\nPosting Date: January 21, 2010\\r\\nLast Updated: October 17, 2016\\r\\n\\r\\nLanguage: English\\r\\n\\r\\nCharacter set encoding: UTF-8\\r\\n\\r\\n*** START OF THIS PROJECT GUTENBERG EBOOK EMMA ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nProduced by An Anonymous Volunteer\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nEMMA\\r\\n\\r\\nBy Jane Austen\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nVOLUME I\\r\\n\\r\\n\\r\\n\\r\\nCHAPTER I\\r\\n\\r\\n\\r\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\r\\nand happy disposition, seemed to unite some of the best blessings of\\r\\nexistence; and had lived nearly twenty-one years in the world with very\\r\\nlittle to distress or vex her.\\r\\n\\r\\nShe was the youngest of the two daughters of a most affectionate,\\r\\nindulg'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['emma',\n",
       "  'woodhouse',\n",
       "  'handsome',\n",
       "  'clever',\n",
       "  'and',\n",
       "  'rich',\n",
       "  'with',\n",
       "  'a',\n",
       "  'comfortable',\n",
       "  'home',\n",
       "  'and',\n",
       "  'happy',\n",
       "  'disposition',\n",
       "  'seemed',\n",
       "  'to',\n",
       "  'unite',\n",
       "  'some',\n",
       "  'of',\n",
       "  'the',\n",
       "  'best',\n",
       "  'blessings',\n",
       "  'of',\n",
       "  'existence',\n",
       "  'and',\n",
       "  'had',\n",
       "  'lived',\n",
       "  'nearly',\n",
       "  'twentyone',\n",
       "  'years',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world',\n",
       "  'with',\n",
       "  'very',\n",
       "  'little',\n",
       "  'to',\n",
       "  'distress',\n",
       "  'or',\n",
       "  'vex',\n",
       "  'her'],\n",
       " ['she',\n",
       "  'was',\n",
       "  'the',\n",
       "  'youngest',\n",
       "  'of',\n",
       "  'the',\n",
       "  'two',\n",
       "  'daughters',\n",
       "  'of',\n",
       "  'a',\n",
       "  'most',\n",
       "  'affectionate',\n",
       "  'indulgent',\n",
       "  'father',\n",
       "  'and',\n",
       "  'had',\n",
       "  'in',\n",
       "  'consequence',\n",
       "  'of',\n",
       "  'her',\n",
       "  'sister’s',\n",
       "  'marriage',\n",
       "  'been',\n",
       "  'mistress',\n",
       "  'of',\n",
       "  'his',\n",
       "  'house',\n",
       "  'from',\n",
       "  'a',\n",
       "  'very',\n",
       "  'early',\n",
       "  'period']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = raw [679:] # gets rid of meta information at the beginning\n",
    "\n",
    "# A few modifications before sentence segmentation\n",
    "text = text.replace('Mr.', 'Mr')\n",
    "text = text.replace('Mrs.', 'Mrs')\n",
    "text = text.replace('\\n', ' ')\n",
    "text = text.replace('\\r', ' ')\n",
    "\n",
    "# Sentence segmentation\n",
    "import re\n",
    "sent_boundary_pattern = r'[.?!]'\n",
    "sentences = re.split(sent_boundary_pattern, text)\n",
    "\n",
    "# Remove punctuation, special characters and upper cases\n",
    "from string import punctuation\n",
    "special = ['“', '”']\n",
    "sentences = [''.join([ch for ch in sent if ch not in punctuation and ch not in special]) for sent in sentences]\n",
    "sentences = [sent.lower() for sent in sentences]\n",
    "\n",
    "# Remove white sace\n",
    "sentences = [sent.strip() for sent in sentences]\n",
    "\n",
    "# Tokenization within sentence\n",
    "list_of_list = [sent.split() for sent in sentences]\n",
    "list_of_list[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a skip-gram model with Word2Vec \n",
    "\n",
    "First, you'll need to install [Gensim](https://pypi.org/project/gensim/). You can do so directly in the notebook using     ```!pip install```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\gregor von richthof\\anaconda3\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in c:\\users\\gregor von richthof\\anaconda3\\lib\\site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\gregor von richthof\\anaconda3\\lib\\site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\gregor von richthof\\anaconda3\\lib\\site-packages (from gensim) (1.16.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\gregor von richthof\\anaconda3\\lib\\site-packages (from gensim) (0.19.0)\n",
      "Requirement already satisfied: boto3 in c:\\users\\gregor von richthof\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim) (1.9.47)\n",
      "Requirement already satisfied: bz2file in c:\\users\\gregor von richthof\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
      "Requirement already satisfied: requests in c:\\users\\gregor von richthof\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim) (2.14.2)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\users\\gregor von richthof\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim) (2.46.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\gregor von richthof\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in c:\\users\\gregor von richthof\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.47 in c:\\users\\gregor von richthof\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.47)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in c:\\users\\gregor von richthof\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.47->boto3->smart-open>=1.2.1->gensim) (2.6.0)\n",
      "Requirement already satisfied: docutils>=0.10 in c:\\users\\gregor von richthof\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.47->boto3->smart-open>=1.2.1->gensim) (0.13.1)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.20; python_version >= \"3.4\" in c:\\users\\gregor von richthof\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.47->boto3->smart-open>=1.2.1->gensim) (1.24.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gregor von Richthof\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\Gregor von Richthof\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2397717, 3378065)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(min_count=2, size=30, sg=1)\n",
    "model.build_vocab(list_of_list)  # prepare the model vocabulary\n",
    "model.train(list_of_list, total_examples=model.corpus_count, epochs=model.iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asses model accuracy\n",
    "### Size of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10244\n"
     ]
    }
   ],
   "source": [
    " print(len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.22904672 -0.28888452  0.6314822   0.5529911  -0.5816087   0.37747845\n",
      "  0.7699377   0.39877328 -1.1237882   0.07456231 -0.6187242   0.10058929\n",
      "  0.3926051   0.81608933 -1.0285398   0.20173627 -0.22362886 -0.06954614\n",
      " -0.33557674  0.7103051   0.02760127  1.1307968  -0.2736073   0.3345016\n",
      "  0.21023332  0.43452004 -0.11738215 -0.17811768  0.01778343  0.648622  ]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.word_vec('woman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5600547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gregor von Richthof\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('woman', 'tree'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most similar words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('man', 0.9755523204803467), ('young', 0.8714357614517212), ('gentlemanlike', 0.8711866736412048), ('girl', 0.8699371814727783), ('wellmeaning', 0.8445643782615662), ('respectable', 0.8269561529159546), ('pleasing', 0.8267240524291992), ('lovely', 0.824920654296875), ('sensible', 0.8242414593696594), ('fellow', 0.8217689990997314)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gregor von Richthof\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similar_by_word('woman'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('husband', 0.9566090106964111), ('sister', 0.8778444528579712), ('aunt', 0.8750278949737549), ('father', 0.8676889538764954), ('mother', 0.8603571057319641), ('niece', 0.8498364090919495), ('encouraging', 0.847824215888977), ('friend', 0.8429862260818481), ('isabella', 0.8417069911956787), ('kindness', 0.8360919952392578)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gregor von Richthof\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "vector = model.wv.word_vec('woman') - model.wv.word_vec('man') + model.wv.word_vec('husband') #wife?\n",
    "print(model.wv.similar_by_vector(vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Try to improve the model by tuning its parameters:\n",
    "- Increase the context window\n",
    "- Construct continuous-bag-of-words representations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
